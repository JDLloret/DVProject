---
title: "Home Assignment: Exploratory analysis and visualization"
output: html_document
date: "2025-05-15"
author: "Joan Lloret and Martí Díez"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load libraries

```{r libraries, message=FALSE}
library(ggplot2)
library(ggfortify)
library(factoextra)
library(stats)
library(Rtsne)
library(dplyr)
library(tidyr)
library(patchwork)
library(gridExtra)
library(umap)
library(ggpubr)
```

# QUESTION 1: Load, adapt the data and create metadata
# Load the data files

```{r data}
rep1 <- read.csv("./data/rep1.csv")
rep2 <- read.csv("./data/rep2.csv")
rep3 <- read.csv("./data/rep3.csv")
rep4 <- read.csv("./data/rep4.csv")
```

# Check the dimensions of the data, fix rows and transpose the matrix to ensure the data is properly displayed

```{r}
rows <- nrow(rep1)
columns <- ncol(rep1)
cat("The data set rep1 has", rows, "rows and", columns, "columns.\n")

rows <- nrow(rep2)
columns <- ncol(rep2)
cat("The data set rep2 has", rows, "rows and", columns, "columns.\n")

rows <- nrow(rep3)
columns <- ncol(rep3)
cat("The data set rep3 has", rows, "rows and", columns, "columns.\n")

rows <- nrow(rep4)
columns <- ncol(rep4)
cat("The data set rep4 has", rows, "rows and", columns, "columns.\n")


rownames(rep1) <- rep1$X
rep1$X <- NULL

rownames(rep2) <- rep2$X
rep2$X <- NULL

rownames(rep3) <- rep3$X
rep3$X <- NULL

rownames(rep4) <- rep4$X
rep4$X <- NULL

all_equal <- all(rownames(rep1) == rownames(rep2)) &&
             all(rownames(rep1) == rownames(rep3)) &&
             all(rownames(rep1) == rownames(rep4))
cat("Gene name order identical across datasets:", all_equal, "\n")


rep1 <- t(rep1)
rep2 <- t(rep2)
rep3 <- t(rep3)
rep4 <- t(rep4)

```

# Create the metadata and bind the datasets for posterior analysis

```{r}
replicate <- rep(c("1", "2", "3", "4"), each = nrow(rep1))

A_B_vector <- rep(c("A", "B"), each = nrow(rep1) / 2)

time_stamp <- rep(c("T1", "T2", "T3", "T4", "T5", "T6"), each=1) 


metadata <- data.frame(replicate, A_B_vector, time_stamp)

data <- rbind(rep1, rep2, rep3, rep4)
```



# QUESTION 2: PCA representation
PCA is a linear method, that aims to preserve the global structure of the data, it's sensitive to magnitude and returns always the same result for the same data (is deterministic)

There are three types of PCA representations:
Unscaled, Scaled and Normalized. The unscaled representation uses raw data and can lead to an uneven representation since it can emphasize genes with higher overall expression.

The scaled representation scales and centers the data and highlights changes across conditions.

The normalized representation adjusts for the total expression. This means that each sample has a similar total expression level. This representation is used when we compare samples where the total expression might vary a lot. 
```{r}
set.seed(123)

# UNSCALED PCA
pca_unscaled <- prcomp(data, scale = FALSE)
names(pca_unscaled)

# SCALED PCA
pca_scaled <- prcomp(data, scale = TRUE)
names(pca_scaled)

# NORMALIZE THE DATA
sum_data <- rowSums(data)
normalized_data <- sweep(data, 1, sum_data, "/")

# NORMALIZED PCA
pca_normalized <- prcomp(normalized_data, scale = TRUE)
```


# Plot the PCA representations

# Unscaled PCA plot
```{r}
pca1_unscaled <- fviz_pca_ind(pca_unscaled,
                              geom.ind = "point",
                              habillage = metadata$A_B_vector,
                              addEllipses = TRUE,
                              title = "A/B experiment")

pca2_unscaled <- fviz_pca_ind(pca_unscaled,
                              geom.ind = "point",
                              habillage = metadata$replicate,
                              addEllipses = TRUE,
                              title = "Replicates")
pca3_unscaled <- fviz_pca_ind(pca_unscaled,
                              geom.ind = "point",
                              habillage = metadata$time_stamp,
                              addEllipses = TRUE,
                              title = "Time")

pca1_unscaled + pca2_unscaled + pca3_unscaled +plot_annotation("Unscaled plots")



```

Here we can see first the A/B experiment differences, where they have opposite growths in relation to Dim2, therefore we expect them to be the main difference within the data. 
After we observe  all the replicates in separate clusters, with exception of group 3 (this may be caused by an outlier mentioned in the data information).
Finally the time variable doesn't show any clear difference between time stamps, showing that its impact will be minimal.
These ellipses helps us see if the data is clustered based on the replicate or the experiment, which could show a possible batch effects.


# Plot full unscaled PCA with ggplot

```{r}
ggplot(pca_unscaled$x, 
       aes(x = PC1, 
           y = PC2, 
           shape = metadata$replicate, 
           size = metadata$A_B_vector, 
           color = metadata$time_stamp))+
  geom_point()+
  scale_shape_manual(values = c(1, 5, 10, 15))+
  scale_size_manual( values = c(3, 5))+
scale_color_brewer(palette = "Set1")+
    labs(title="Unscaled PCA", 
         size = "Type", 
         shape = "Replicate", 
         color = "Time")+
  theme_minimal()



```

This plot reinforces the previous statement, showing how the samples group by experiment (PC2) and replicates (PC1) but the group times are separated, expressing their small effect over the samples. The red crossed circle is an outlier that we will come back later on.


# Scaled PCA plot


```{r}
pca1_scaled <- fviz_pca_ind(pca_scaled,
                              geom.ind = "point",
                              habillage = metadata$A_B_vector,
                              addEllipses = TRUE,
                              title = "A/B experiment")

pca2_scaled <- fviz_pca_ind(pca_scaled,
                              geom.ind = "point",
                              habillage = metadata$replicate,
                              addEllipses = TRUE,
                              title = "Replicates")

pca3_scaled <- fviz_pca_ind(pca_scaled,
                              geom.ind = "point",
                              habillage = metadata$time_stamp,
                              addEllipses = TRUE,
                              title = "Time")


pca1_scaled + pca2_scaled+ pca3_scaled + plot_annotation("Scaled plots")

```

Once scaled we still remain in a similar situation this is not unusual for a PCA, where scaling may not cause differences, still scaling is an essential step to ensure the results are correct. 
In this case the scaling has adapted a bit to take into account big value discrepancies, but the data still needs to be normalized.

# Plot full scaled PCA plot with ggplot


```{r}
ggplot(pca_scaled$x,
       aes(x = PC1,
           y = PC2, 
           shape = metadata$replicate,
           size = metadata$A_B_vector,
           color = metadata$time_stamp))+
  geom_point()+
  scale_shape_manual(values = c(1, 5, 10, 15))+
  scale_size_manual( values = c(3, 5))+
  scale_color_brewer(palette = "Set1")+
    labs(title = "Scaled PCA", 
         size = "Type", 
         shape = "Replicate", 
         color = "Time")+
  theme_minimal()

```
Here we can see that it stays mostly the same as before. Using this type of data reduces the samples with high variance, and allows us to focus more on the differences in proportion. 
With scaling there isn't as much batch effect, or at least it is more subtle when looking for differences between samples. 

# Normalized PCA plot

```{r}
pca1_normalized <- fviz_pca_ind(pca_normalized,
                              geom.ind = "point",
                              habillage = metadata$A_B_vector,
                              addEllipses = TRUE,
                              title = "A/B experiment")

pca2_normalized <- fviz_pca_ind(pca_normalized,
                              geom.ind = "point",
                              habillage = metadata$replicate,
                              addEllipses = TRUE,
                              title = "Replicates")

pca3_normalized <- fviz_pca_ind(pca_normalized,
                              geom.ind = "point",
                              habillage = metadata$time_stamp,
                              addEllipses = TRUE,
                              title = "Time")


pca1_normalized + pca2_normalized+ pca3_normalized + plot_annotation("Normalized plots")

```

After normalizing we now see bigger changes, but they seem to be caused due to the group B and 3 time 1 outlier we saw before that makes the whole plot shift, therefore we will have to address it.


# Plot full normalized PCA with ggplot
```{r}
ggplot(pca_normalized$x,
       aes(x = PC1,
           y = PC2, 
           shape = metadata$replicate,
           size = metadata$A_B_vector,
           color = metadata$time_stamp))+
  geom_point()+
  scale_shape_manual(values = c(1, 5, 10, 15))+
  scale_size_manual( values = c(3, 5))+
  scale_color_brewer(palette = "Set1")+
    labs(title = "Normalized PCA", 
         size = "Type", 
         shape = "Replicate", 
         color = "Time")+
  theme_minimal()


```
Here we see how extremely this outlier conditions our data, leaving only the groups as a visible difference while it stays greatly distantiated.

# Outliers

We need to identify the outliers since they can skew our results. What we'll do is find the max value in all PCAs to find this outlier and remove it from our data to then normalize again to get more accurate results. 

```{r}

# Find outliers
point <- data.frame(pca_normalized$x)

absolute_point <- abs(point)

max_value <- max(absolute_point, na.rm = TRUE)

max_pos <- which(absolute_point == max_value, arr.ind = TRUE)

rowname <- rownames(point)[max_pos[1]]
colname <- colnames(point)[max_pos[2]]


# Remove outliers
data <- data[-max_pos[1],]
metadata <- metadata[-max_pos[1],]


# Normalize again
set.seed(123)
sum_data <- rowSums(data)
normalized_data <- sweep(data, 1, sum_data, "/")

pca_normalized <- prcomp(normalized_data, scale = TRUE)
```

# Plot full normalized PCA without outliers with ggplot
```{r}
ggplot(pca_normalized$x,
       aes(x = PC1,
           y = PC2, 
           shape = metadata$replicate,
           size = metadata$A_B_vector,
           color = metadata$time_stamp))+
  geom_point()+
  scale_shape_manual(values = c(1, 5, 10, 15))+
  scale_size_manual( values = c(3, 5))+
  scale_color_brewer(palette = "Set1")+
    labs(title = "Normalized PCA", 
         size = "Type", 
         shape = "Replicate", 
         color = "Time")+
  theme_minimal()


```
Now we can more clearly see how the data is distributed, the replicates remain in groups.
Type shows the most variance (As seen with the distance between type in the X axis).
After we can see how time resulted more relevant than expected since the clusters are of replicate and time is what creates discrepancies in the Y axis. 

Still with the results above we could determine that the type of yeast is the main responsible for the taste differences, followed by the time of fermentation.

# tSNE representation

t-SNE is a non-linear method that aims to preserve the local structure of the data. It is sensitive to input scaling and randomness, meaning it can return slightly different results unless a random seed is fixed (it is non-deterministic).

Unscaled, as PCA simply works with the raw data.
On the other hand scaling is always essential in tSNE as well as taking into account the iterations and perplaxity.

Finally, normalizing aims to remove potential bias in the data.

```{r}
maximum_perplexity <- (nrow(normalized_data) - 1) / 3 

# Unscaled tSNE
unscaled_tsne <- Rtsne(data, dims = 2, perplexity = maximum_perplexity/2) 

# If we add verbose = TRUE it will tell us the error value for each 50 iterations until the maximum iterations default value, but it is unnecessary text. The tSNE is performed well for each case.

unscaled_tsne_data <- data.frame(unscaled_tsne$Y, 
                                 replicate = metadata$replicate, A_B_vector = metadata$A_B_vector, time_stamp = metadata$time_stamp)

colnames(unscaled_tsne_data) <- c("Dim1", "Dim2", "Replicate", "Type", "Time")


# Scaled tSNE
scaled_data <- scale(data)

scaled_tsne <- Rtsne(scaled_data, dims = 2, perplexity = maximum_perplexity/2)

scaled_tsne_data <- data.frame(scaled_tsne$Y, replicate = metadata$replicate, A_B_vector = metadata$A_B_vector, time_stamp = metadata$time_stamp)
colnames(scaled_tsne_data) <- c("Dim1", "Dim2", "Replicate", "Type", "Time")


#Normalized
sum_data <- rowSums(data)
normalized_data <- sweep(data, 1, sum_data, "/")

normalized_tsne <- Rtsne(normalized_data, dims = 2, 
                         perplexity = maximum_perplexity/2)

normalized_tsne_data <- data.frame(normalized_tsne$Y, 
                                   replicate = metadata$replicate, A_B_vector = metadata$A_B_vector, time_stamp = metadata$time_stamp)

colnames(normalized_tsne_data) <- c("Dim1", "Dim2", "Replicate", "Type", "Time")
```

# tSNE plots
# Unscaled tSNE plot

```{r}
ggplot(unscaled_tsne_data, aes(x = Dim1, y = Dim2, shape = Replicate, size = Type, color = Time))+
  geom_point()+
  scale_shape_manual(values = c(1, 5, 10, 15))+
  scale_size_manual( values = c(3, 5))+
  scale_color_brewer(palette = "Set1")+
    labs(title = "Unscaled tSNE plot", 
         size = "Type", 
         shape = "Replicate", 
         color = "Time")+
  theme_minimal()


```

This plot shows that the samples cluster based on replicate and type, making time the discrepancy within clusters. There isn’t a clear separation between experiment types (A or B), since those who share replicate are still fairly near.

# Scaled tSNE plot

```{r}
ggplot(scaled_tsne_data, aes(x = Dim1, y = Dim2, shape = Replicate, size = Type, color = Time))+
  geom_point()+
  scale_shape_manual(values = c(1, 5, 10, 15))+
  scale_size_manual( values = c(3, 5))+
  scale_color_brewer(palette = "Set1")+
    labs(title = "Scaled tSNE plot", 
         size = "Type", 
         shape = "Replicate", 
         color = "Time")+
  theme_minimal()

```

Once scaled we see the samples clearly grouped by replicate, with overlapping, and also the wine type separation has diminished in comparasion to the unscaled plot. This variates from what we saw at the PCA, where there was a clear distance between groups


# Normalized tSNE plot

```{r}
ggplot(normalized_tsne_data, aes(x = Dim1, y = Dim2, shape = Replicate, size = Type, color = Time))+
  geom_point()+
  scale_shape_manual(values = c(1, 5, 10, 15))+
  scale_size_manual( values = c(3, 5))+
  scale_color_brewer(palette = "Set1")+
    labs(title = "Normalized tSNE plot", 
         size = "Type", 
         shape = "Replicate", 
         color = "Time")+
  theme_minimal()

```
In the normalized tSNE plot, there is a shift, the distance between replicates becomes the biggest change. Now we can see clusters formed by samples that sheare time and type, leaving groups with one of each possible replicate. 
This is due to normalization removing the sample bias.

# tSNE parameters

We'll use the normalized dataset

# Reproducibility

```{r reproducibility}

plot_reproducibility <- function(normalized_data, metadata){
  
  maximum_perplexity <- (nrow(normalized_data) - 1) / 3
  data <- Rtsne(normalized_data, perplexity = maximum_perplexity / 2)
  
  tsne_plots <- ggplot(data$Y, aes(x=data$Y[,1], y=data$Y[,2], shape=metadata$replicate, size = metadata$A_B_vector, color = metadata$time_stamp))+
    geom_point()+  scale_shape_manual(values = c(1, 5, 10, 15))+
  scale_size_manual( values = c(3, 5))+
  scale_color_brewer(palette = "Set1")+
    labs(x="tSNE 1", y="tSNE 2", shape="Replicate", size = "Type", color = "Time")+
    theme_minimal()
  
  return(tsne_plots)
}


reproducibility_values <- list()

for (i in 1:4){
  
  rep_plot <- plot_reproducibility(normalized_data, metadata)+ggtitle(paste0(i, " tSNE repetition"))
  reproducibility_values[[i]] <- rep_plot
}

#Extract legend
reproducibility_legend<-get_legend(reproducibility_values[[1]])

no_legend_reproducibility <- lapply(reproducibility_values, "+", theme(legend.position = "none"))

grid.arrange(
  arrangeGrob(grobs = no_legend_reproducibility, ncol = 2),
  reproducibility_legend,
  ncol=2,
  widths = c(10, 1),
  top="tSNE Repetitions"
)

```

Since tSNE is non deterministic, if we don't set a seed each repetition gives a different graph, but as we can see the clusters are consistenly composed of one of each replicate, with two distant and bigger groups being the type


# Perplexity
```{r perplexity}


PerplexityPlotter <- function(normalized_data, metadata, pr){
  
  set.seed(123)
  model <- Rtsne(normalized_data, perplexity = pr)
  tsne_plots <- ggplot(model$Y, aes(model$Y[,1],
                       model$Y[,2],
                       shape = metadata$replicate,
                       size = metadata$A_B_vector, color = metadata$time_stamp))+
    geom_point()+
    scale_shape_manual(values = c(1, 5, 10, 15))+
  scale_size_manual( values = c(3, 5))+
  scale_color_brewer(palette = "Set1")+
    labs(x = "tSNE 1", y = "tSNE 2", shape = "Replicate", size = "Type", color = "Time", title=paste0("tSNE plot for perplexity=", round(pr, 2)))
  return(tsne_plots)
}

maximum_perplexity <- (nrow(normalized_data) - 1) / 3 

test_perplexity_values <- c(1, maximum_perplexity/3, (maximum_perplexity/3) * 2, maximum_perplexity)

perplexity_plots <- list()

for (i in seq_along(test_perplexity_values)){
  per_val <- test_perplexity_values[i]
  per_plot <- PerplexityPlotter(normalized_data, metadata, per_val)
  perplexity_plots[[i]] <- per_plot
}

perplexity_legend <- get_legend(perplexity_plots[[1]])

no_legend_perplexity <- lapply(perplexity_plots, "+", theme(legend.position = "none"))

grid.arrange(
  arrangeGrob(grobs = no_legend_perplexity, ncol = 2),
  perplexity_legend,
  ncol = 2,
  widths = c(10, 1),
  top = "tSNE Perplexity"
)


```


Low perplexity values will make clusters too tight/defined. When raising the perplexity value, these clusters reorganize themselves into less number of clusters with more samples in each cluster. In the maximum perplexity value we see the extreme case which is 2 clusters really far apart from each other.

We determined that the optimal perplexity value is arround 5



# Iterations

```{r iterations}

plot_iterations<-function(normalized_data, metadata, num_iterations){
  set.seed(123)
  
  maximum_perplexity <- (nrow(normalized_data)-1)/3
  data <- Rtsne(normalized_data, perplexity = maximum_perplexity / 2, max_iter = num_iterations)

  tsne_plots <- ggplot(data$Y, aes(x=data$Y[,1], y=data$Y[,2], 
                          shape = metadata$replicate, size = metadata$A_B_vector, color = metadata$time_stamp))+
    geom_point()+
    scale_shape_manual(values = c(1, 5, 10, 15))+
  scale_size_manual( values = c(3, 5))+
  scale_color_brewer(palette = "Set1")+
    labs(x="tSNE 1", y="tSNE 2", shape = "Replicate", size = "Type", color = "Time",
         title=paste0(num_iterations, " iterations tSNE plot"))
  return(tsne_plots)
}

iterations<-c(50, 600, 1000, 10000)

iterations_plots <- list()

for (i in seq_along(iterations)){
  iter_num <- iterations[i]
  iter_plot <- plot_iterations(normalized_data, metadata, iter_num)
  iterations_plots[[i]] <- iter_plot
}

iterations_legend <- get_legend(iterations_plots[[1]])

no_legend_iterations <- lapply(iterations_plots, "+", theme(legend.position = "none"))

grid.arrange(
  arrangeGrob(grobs = no_legend_iterations, ncol = 2),
  iterations_legend,
  ncol=2,
  widths = c(10, 1),
  top="tSNE Iterations"
)

```

Since the number of iterations controls how long the algorithm refines the embedding, running fewer iterations can result in an underdeveloped structure(as shown at 50 iterations where the samples show no tendency nor clusters), while more iterations allow the algorithm to better separate and position the points. 
At 600 iterations can clearly see the expected clusters (By type and time) whereas in iterations 1000 and 10000, the clusters are too highly distinct on the type and don't show all the information available. 




# UMAP Representation
UMAP is faster than tSNE and preserves the global structure better
# UMAP plots
```{r}
plot_umap <- function(data, metadata){
  umap_plots <- ggplot(data$layout, aes(x = data$layout[,1], y = data$layout[,2], shape = metadata$replicate, size = metadata$A_B_vector, color = metadata$time_stamp))+
    geom_point()+
    scale_shape_manual(values = c(1, 5, 10, 15))+
    scale_size_manual( values = c(3, 5))+
    scale_color_brewer(palette = "Set1")+
    labs(x = "X", y = "Y", shape = "Replicate", size = "Type", color = "Time")+
    theme_minimal()
  
  return(umap_plots)
}
```


# Raw data UMAP plot
```{r}
umap_raw_plot <- umap(data)

plot_umap(umap_raw_plot, metadata)+
  labs(title = "UMAP Raw data plot")


```
As with PCA and tSNE unscaled we see that there are clusters made by Type and replicate. 
But as we can see there is a huge distance with one particular cluster that seems to condition the data (replicate 4 type B). We will normalize the data to correct this possible issue.

# Normalized data UMAP plot
```{r}
umap_normalized_plot <- umap(normalized_data)

plot_umap(umap_normalized_plot, metadata)+
  labs(title = "UMAP Normalized data plot")

```
Once normalized in UMAP we see a clear shift. Like PCA and tSNE the Type represents the biggest distance between samples and closely grouped by time. But still we see a distance between replicates 4 and the rest of the group we expected
# Predict the UMAP
```{r}
umap_training <- metadata$replicate %in% c("1", "2", "3")

umap_rep4_data <- normalized_data[umap_training, ]

metadata_train <- metadata[umap_training, ]

umap_train <- umap(umap_rep4_data)

plot_umap(umap_train, metadata_train)+
  labs(title = "UMAP data Replicates 1-3")


```
If we look at the data without the fourth replicate we can see that the distance within time  clusters is reduced significantly.


```{r}
rep4_umap <- predict(umap_train, rep4)

rep4_metadata <- metadata[!umap_training, ]

rep4_data <- data.frame(Dim1 = rep4_umap[,1], Dim2 = rep4_umap[,2], Type = rep4_metadata$A_B_vector, Time = rep4_metadata$time_stamp)


ggplot(rep4_data, aes(x = Dim1, y = Dim2, shape = Type, color = Time))+
  geom_point()+
  scale_shape_manual(values = c(1, 5, 10, 15))+
  scale_color_brewer(palette = "Set1")+
  labs(title = "Prediction for Replicate 4")+
  theme_minimal()


```
And by showing only replicate 4 we clearly see the issue being Type B time 3, greatly distanced from the rest. We'll have to work around it.
# Parameter exploration: n_neighbors
```{r warning=FALSE}
neighbor_plotter <- function(data, metadata, num){
  n_neigh_umap <- umap(data, n_neighbors = num)
  umap_plot <- plot_umap(n_neigh_umap, metadata)
  umap_plot <- umap_plot+
    labs(title=paste0("Neighbors: ", num))
  
  return(umap_plot)
}

neighbor_plots <- list()

neighbors <- seq(2, nrow(normalized_data), by = 10)

for (i in seq_along(neighbors)){
  num <- neighbors[i]
  num_plot <- neighbor_plotter(normalized_data, metadata, num)
  neighbor_plots[[i]] <- num_plot
}

neighbor_legend <- get_legend(neighbor_plots[[1]])

no_legend_neighbor <- lapply(neighbor_plots, "+", theme(legend.position = "none"))

grid.arrange(
  arrangeGrob(grobs = no_legend_neighbor, ncol = 2),
  neighbor_legend,
  ncol = 2,
  widths = c(10, 1),
  top="UMAP Neighbors"
)
```
Here we can see the effect of changing neighbour values in UMAP. 

At two we see a lot of clusters but cant get a relationship between them. This is because a low values it focuses on showing clear and several clusters, over-splitting them and not allowing to get the information we aim for.
At twelve we see that there is still distance between two clusters of type B we don't expect.

And finally after twenty two there are two clusters determined by type. Since here the aim is for two clusters to be the main result and explanation it seems to be no problem, but if we over increased the neighbours we would miss relevant information within clusters.

# Parameter exploration: min_dist
```{r warning=FALSE}

min_dist_plotter <- function(data, metadata, dist_value){
  
  min_dist_umap <- umap(data, min_dist = dist_value)
  umap_plot <- plot_umap(min_dist_umap, metadata)
  umap_plot <- umap_plot+
    labs(title=paste0("Minimum distance: ", dist_value))
  
  return(umap_plot)
  
}

min_dist_list <- list()

distances <- c(0.1, 0.3, 0.5, 0.7, 0.9)

for (i in seq_along(distances)){
  min <- distances[i]
  min_dist_plot <- min_dist_plotter(normalized_data, metadata, min)
  min_dist_list[[i]] <- min_dist_plot
}

min_dist_legend <- get_legend(min_dist_list[[1]])

no_legend_min_dist <- lapply(min_dist_list, "+", theme(legend.position = "none"))

grid.arrange(
  arrangeGrob(grobs = no_legend_min_dist, ncol = 2),
  min_dist_legend,
  ncol = 2,
  widths = c(10, 1),
  top="UMAP Minimum Distance"
)


```
Here we see the effect of minimum distance, which determines the minimum distance allowed between samples in the UMAP.

This aims to determine the focus of the plot, where low distance shows more differences within groups (We see until Md 0.5 individuality of times 1 and 2 of Type A that are lost with a higher distance), but at higher value we see more the distance between Types, while losing time distinctions. 


# Parameter exploration: n_epochs
```{r}
epoch_plotter <- function(data, metadata, epoch){
  epoch_umap <- umap(data, n_epochs = epoch)
  umap_plot <- plot_umap(epoch_umap, metadata)
  umap_plot <- umap_plot+
    labs(title=paste0("Number of iterations: ", epoch))
  
  return(umap_plot)
}


epoch_list <- list()

epoch_values <- c(100, 500, 1000, 10000)

for (i in seq_along(epoch_values)){
  num <- epoch_values[i]
  epoch_plot <- epoch_plotter(normalized_data, metadata, num)
  epoch_list[[i]] <- epoch_plot
}

epoch_legend <- get_legend(epoch_list[[1]])

no_legend_epoch <- lapply(epoch_list, "+", theme(legend.position = "none"))

grid.arrange(
  arrangeGrob(grobs = no_legend_epoch, ncol = 2),
  epoch_legend,
  ncol=2,
  widths = c(10, 1),
  top="UMAP Number of iterations"
)

```
We see that the change of UMAP by changing the iterations is not nearly as impactfull as in tSNE, but still at higher values we start having too much overlapping


# Final interpretation

We'll use the normalized data with outliers removed since it is the one that represents our data most accurately


# Final PCA
```{r final_pca}
final_pca1 <- ggplot(pca_normalized$x,
                     aes(x = PC1, y = PC2, shape = metadata$replicate, size = metadata$A_B_vector, color = metadata$time_stamp))+
  geom_point()+
  scale_shape_manual(values = c(1, 5, 10, 15))+
  scale_size_manual( values = c(3, 5))+
  scale_color_brewer(palette = "Set1")+
    labs(size = "Type", 
         shape = "Replicate", 
         color = "Time")+
  theme_minimal()



final_pca2 <- fviz_pca_ind(pca_normalized,              geom.ind = "point", 
             habillage = metadata$A_B_vector,
             addEllipses = TRUE) +
  labs(x = "PC1", y = "PC2")+
  theme_minimal()

final_pca1 + final_pca2 + plot_annotation(title = "Final PCA representation with accurate data")

```


We observe that our data has been grouped into two clusters, which are classified by the type of experiment (not the replicate where they come from) and this means that the variance that is seen between PC1 and PC2 has to be explained by biological factors. Since the groups are clearly separated, we can confirm that the batch effect has been eliminated.

# Final tSNE representation


```{r}
set.seed(123)

maximum_perplexity <- (nrow(normalized_data) - 1) / 3

ggplot(normalized_tsne_data, aes(x = Dim1, y = Dim2, shape = metadata$replicate, size = metadata$A_B_vector, color = metadata$time_stamp)) +
  geom_point() +
  scale_shape_manual(values = c(1, 5, 10, 15))+
  scale_size_manual( values = c(3, 5))+
  scale_color_brewer(palette = "Set1")+
  labs(title = "Final tSNE representation with accurate data", 
       x = "Dim1", y = "Dim2", shape = "Replicate", size = "Type", color = "Time") +
  theme_minimal()


```

The t-SNE plot shows how samples group based on how similar they are, and it helps support what we saw with the PCA. In our plot, the samples are grouped more by their experimental type (A/B) than what replicate they came from. This suggests that the differences in the data are due to real biological effects, not technical issues like batch effects. This supports the PCA representation since both PCA and tSNE are clustered the same way.


# Final UMAP representation
```{r warning=FALSE}
final_umap <- umap(normalized_data, n_epochs = 500, min_dist = 0.3, n_neighbors = 22)

plot_umap(final_umap, metadata)+
  labs(title = "Final UMAP representation with accurate data")

```
Here, UMAP shows the distance between type even stronger than the others. confirming that the main difference is caused by the type of yeast.

# Final interpretation

PCA, tSNE and UMAP all bring us to the same conclusion: The type of yeast is by a large margin the main factor that differentiates the wines.

PCA showed clear clusters in both type and time, with Type being still significantly stronger.

tSNE showed a similar pattern, especially after adjusting the settings for the plot to be optimal. The distance between types in tSNE seems to be the smallest, while showcasing more the effects timestamps cause to the samples.

On the other hand UMAP showcases the distance between Types in an even more exaggerated way, but also loses time distinctions.

In conclusion, all methods brought us to the same result: The genetic differences in the yeast cause the main difference in taste of the wines. Still we were able to make different observations with each method, showcasing that using multiple method to contrast the data is useful.

